<!DOCTYPE html>
<html>

<head>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="google-site-verification" content="ONL58rZnCLtdQDM-rXI9vLreOYlM_esgpUYTRUQsUzM" />
	<title>Nissim Maruani</title>
	<link rel="icon" href="img/favicon.png">

	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@200&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@500&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&family=Space+Mono:ital@1&display=swap"
		rel="stylesheet">
	<link href="style.css" rel="stylesheet">
</head>

<body>
	<div class="container">
		<img class="profile-img" src="img/profile.jpeg" alt="Profile Picture">
		<div>
			<h1>Nissim Maruani</h1>
			<p class="contact"> <a class="custom-link" href="mailto:nissim.maruani@inria.fr">Contact</a> | <a
					class="custom-link" href="https://www.github.com/nissmar"> GitHub</a> | <a
					class="custom-link" href="https://scholar.google.com/citations?user=ae3FZA0AAAAJ&hl=fr&authuser=1"> Scholar</a> | <a class="custom-link"
					href="https://www.linkedin.com/in/nissim-maruani/"> LinkedIn</a> </p>
			<div class="bio">
				<p>I am a third year PhD student at INRIA, under the supervision of <a class="custom-link"
						href="https://team.inria.fr/titane/pierre-alliez/">Pierre Alliez</a> (<a class="custom-link"
						href="https://www.inria.fr/fr/titane">Titane team</a>) and <a class="custom-link"
						href="https://pages.saclay.inria.fr/mathieu.desbrun/">Mathieu Desbrun</a> (<a
						class="custom-link" href="https://www.inria.fr/fr/geomerix">Geomerix team</a>). My research
					topic combines geometry processing with deep learning. Our goal is to enable neural networks to
					generate efficient 3D shapes in a differentiable pipeline. </p>
			</div>
		</div>
	</div>


	<h2>Publications</h2>

	<!-- <ul class="pub-line"></ul> -->
	<!-- <ul class="pub-line"></ul> -->
	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/projects/shapeshifter">
					<img class="pub-img" src="img/shapeshifter.gif" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/projects/shapeshifter"> ShapeShifter: 3D Variations Using Multiscale and Sparse Point-Voxel
					Diffusion</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu Desbrun </div>

				<div class="pub-abstract">
					This paper proposes a new 3D generative model that learns to synthesize shape variations based on a single
					example. While generative methods for 3D objects have recently attracted much attention, current
					techniques often lack geometric details and/or require long training times and large resources. Our
					approach remedies these issues by combining sparse voxel grids and multiscale point, normal, and color
					sampling within an encoder-free neural architecture that can be trained efficiently and in parallel. We
					show that our resulting variations better capture the fine details of their original input and can capture
					more general types of surfaces than previous SDF-based methods. Moreover, we offer interactive generation
					of 3D shape variants, allowing more human control in the design loop if needed.
				</div>
				<div class="pub-conf"> Proc. Conference on Computer Vision and Patter Recognition (CVPR), 2025</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/projects/ponq">
					<img class="pub-img" src="img/PoNQ.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/projects/ponq"> PoNQ: a Neural QEM-based Mesh
					Representation</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun </div>

				<div class="pub-abstract">
					Although polygon meshes have been a standard representation in geometry processing, their irregular
					and combinatorial nature hinders their suitability for learning-based applications. In this work, we
					introduce a novel learnable mesh representation through a set of local 3D sample Points and their
					associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape, which we denote
					PoNQ. A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the
					local quadric errors. Besides marking the first use of QEM within a neural shape representation, our
					contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh
					does not self-intersect and is always the boundary of a volume. Notably, our representation does not
					rely on a regular grid, is supervised directly by the target surface alone, and also handles open
					surfaces with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ through a
					learning-based mesh prediction from SDF grids and show that our method surpasses recent
					state-of-the-art techniques in terms of both surface and edge-based metrics.
				</div>
				<div class="pub-conf"> Proc. Conference on Computer Vision and Patter Recognition (CVPR), 2024</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">

		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/voromesh.github.io/">
					<img class="pub-img" src="img/Voromesh.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/voromesh.github.io/"> VoroMesh: Learning Watertight
					Surface Meshes with Voronoi Diagrams</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Roman Klokov, Maks Ovsjanikov, Pierre Alliez, Mathieu
					Desbrun </div>

				<div class="pub-abstract">
					In stark contrast to the case of images, finding a concise, learnable discrete representation of 3D
					surfaces remains a challenge. In particular, while polygon meshes are arguably the most common
					surface representation used in geometry processing, their irregular and combinatorial structure
					often make them unsuitable for learning-based applications. In this work, we present VoroMesh, a
					novel and differentiable Voronoi-based representation of water- tight 3D shape surfaces. From a set
					of 3D points (called generators) and their associated occupancy, we define our boundary
					representation through the Voronoi diagram of the generators as the subset of Voronoi faces whose
					two associated (equidistant) generators are of opposite occupancy: the resulting polygon mesh forms
					a watertight approximation of the target shape’s boundary. To learn the position of the generators,
					we propose a novel loss function, dubbed VoroLoss, that minimizes the distance from groundtruth
					surface samples to the closest faces of the Voronoi diagram which does not require an explicit
					construction of the entire Voronoi diagram. A direct optimization of the Voroloss to obtain
					generators on the Thingi32 dataset demonstrates the geometric efficiency of our representation
					compared to axiomatic meshing algorithms and recent learning-based mesh representations. We further
					use VoroMesh in a learning-based mesh prediction task from input SDF grids on the ABC dataset, and
					show comparable performance to state-of-the-art methods while guaranteeing closed output surfaces
					free of self-intersections.
				</div>
				<div class="pub-conf"> Proc. International Conference on Computer Vision (ICCV), 2023</div>

			</div>
		</div>

	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">

		<div class="container">
			<div class="zoomhover">
				<a href="https://inria.hal.science/hal-04135266#">
					<img class="pub-img" src="img/offset.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://inria.hal.science/hal-04135266#"> Feature-Preserving Offset Mesh
					Generation from Topology-Adapted Octrees</a>
				<div class="pub-authors">Daniel Zint, <b>Nissim Maruani</b>, Mael Rouxel-Labbé, Pierre Alliez</div>

				<div class="pub-abstract">We introduce a reliable method to generate offset meshes from input triangle
					meshes or triangle soups. Our method proceeds in two steps. The first step performs a Dual
					Contouring method on the offset surface, operating on an adaptive octree that is refined in areas
					where the offset topology is complex. Our approach substantially reduces memory consumption and
					runtime compared to isosurfacing methods operating on uniform grids. The second step improves the
					output Dual Contouring mesh with an offset-aware remeshing algorithm to reduce the normal deviation
					between the mesh facets and the exact offset. This remeshing process reconstructs concave sharp
					features and approximates smooth shapes in convex areas up to a user-defined precision. We show the
					effectiveness and versatility of our method by applying it to a wide range of input meshes. We also
					benchmark our method on the entire Thingi10k dataset: watertight, 2-manifold offset meshes are
					obtained for 100% of the cases.</div>
				<div class="pub-conf">Symposium on geometry processing (SGP), 2023</div>

			</div>

		</div>
	</ul>
	<!-- <ul class="pub-line"></ul> -->

	<h2>Teaching</h2>
	<ul>
		<li> ENS-PSL (2022-Today, Paris, France): exploring new ways of teaching maths with <a class="custom-link"
				href="https://mathadata.fr/">MathAData</a> </li>
		<li>
			<a class="custom-link" href="https://www.larotonde-sciences.com">La Rotonde</a> (2018-2019, Saint-Étienne,
			France): Science Mediation at <a class="custom-link" href="https://fondation-lamap.org/en">La main à la
				pâte</a>
		</li>
	</ul>

	<h2>Reviewer</h2>
	<ul>
		<li>BMVC 2024, SIGGRAPH ASIA 2024, SIGGRAPH 2024, BMVC 2023</li>
	</ul>

	<h2>Education</h2>
	<ul>
		<li> ENS Paris-Saclay (2021-2022, Gif-sur-Yvette, France): Master MVA</li>
		<li> École polytechnique (2018-2022, Saclay, France): Engineering Curriculum </li>
		<li> Lycée Louis-le-Grand (2016-2018, Paris, France): "Classe prépa" </li>
	</ul>


	<h2>Coding projects</h2>

	<div class="container project">
		<a href="https://github.com/nissmar/ARDM" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/ardm.jpg">
			<p class="code-title custom-link">Diffusion Models</p>
		</a>
		<a href="https://github.com/nissmar/VSA" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/vsa.gif">
			<p class="code-title custom-link">VSA</p>
		</a>
		<a href="https://github.com/nissmar/Learning-Active-Contour" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/segmentation.gif">
			<p class="code-title custom-link">U-Net</p>
		</a>
		<a href="https://github.com/nissmar/Radiance-Fields" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/plenoxels.gif">
			<p class="code-title custom-link">Plenoxels</p>
		</a>
		<a href="https://github.com/nissmar/FilmNoise/" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/film-noise.jpg">
			<p class="code-title custom-link">Analog Grain Simulation</p>
		</a>
		<a href="https://github.com/nissmar/Paper_Plane_VCL/" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/airplane.png">
			<p class="code-title custom-link">Paper Plane Simulation</p>
		</a>
		<a href="https://github.com/nissmar/Bouncing_Lasers" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/lasers.gif">
			<p class="code-title custom-link">Laser Simulation</p>
		</a>
		<a href="https://github.com/nissmar/ForgeryDetection" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/forgery.gif">
			<p class="code-title custom-link">Forgery Detection</p>
		</a>




	</div>



</body>

</html>