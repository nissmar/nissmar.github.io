<!DOCTYPE html>
<html>

<head>
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="google-site-verification" content="ONL58rZnCLtdQDM-rXI9vLreOYlM_esgpUYTRUQsUzM" />
	<title>Nissim Maruani</title>
	<link rel="icon" href="img/favicon.png">

	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@200&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@500&display=swap" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css2?family=Roboto+Slab:wght@600&family=Space+Mono:ital@1&display=swap"
		rel="stylesheet">
	<link href="style.css" rel="stylesheet">
</head>

<body>
	<div class="container">
		<img class="profile-img" src="img/profile.jpeg" alt="Profile Picture">
		<div>
			<h1>Nissim Maruani</h1>
			<p class="contact"> <a class="custom-link" href="mailto:nissim.maruani@inria.fr">Contact</a> | <a
					class="custom-link" href="https://www.github.com/nissmar"> GitHub</a> | <a class="custom-link"
					href="https://scholar.google.com/citations?user=ae3FZA0AAAAJ&hl=fr&authuser=1"> Scholar</a> | <a
					class="custom-link" href="https://www.linkedin.com/in/nissim-maruani/"> LinkedIn</a> </p>
			<div class="bio">
				<p>I'm a third year PhD student at Inria, under the supervision of <a class="custom-link"
						href="https://team.inria.fr/titane/pierre-alliez/">Pierre Alliez</a> (<a class="custom-link"
						href="https://www.inria.fr/fr/titane">Titane team</a>) and <a class="custom-link"
						href="https://pages.saclay.inria.fr/mathieu.desbrun/">Mathieu Desbrun</a> (<a
						class="custom-link" href="https://www.inria.fr/fr/geomerix">Geomerix team</a>). Last summer, I
					interned at <a class="custom-link" href="https://research.adobe.com">Adobe Research</a> with
					mentorship from <a class="custom-link" href="https://yifita.netlify.app">Yifan Wang</a>. My work
					lies at the
					intersection of geometry processing and deep learning, focusing on differentiable geometric
					representations that enable fast and accurate reconstruction. I’m particularly interested in
					data-driven approaches and generative models.</p>
			</div>
		</div>
	</div>


	<h2>Publications</h2>

	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://arxiv.org/abs/2511.17454">
					<img class="pub-img" src="img/illustrators_depth_square.pdf" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://arxiv.org/abs/2511.17454"> Illustrator’s Depth: Monocular Layer Index
					Prediction for Image Decomposition</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Peiying Zhang, Siddhartha Chaudhuri, Matthew Fisher,
					Nanxuan Zhao, Vladimir G. Kim, Pierre Alliez, Mathieu Desbrun, Wang Yifan </div>

				<div class="pub-abstract">
					We introduce Illustrator’s Depth, a novel definition of depth that addresses a key challenge in
					digital content creation: decomposing flat images into editable, ordered layers. Inspired by an
					artist’s compositional process, illustrator’s depth infers a layer index for each pixel, forming an
					interpretable image decomposition through a discrete, globally consistent ordering of elements
					optimized for editability. We also propose and train a neural network using a curated dataset of
					layered vector graphics to predict layering directly from raster inputs. Our layer index inference
					unlocks a range of powerful downstream applications. In particular, it significantly outperforms
					state-of-the-art baselines for image vectorization while also enabling high-fidelity
					text-to-vector-graphics generation, automatic 3D relief generation from 2D images, and intuitive
					depth-aware editing. By reframing depth from a physical quantity to a creative abstraction,
					illustrator's depth prediction offers a new foundation for editable image decomposition.
				</div>
				<div class="pub-conf">Arxiv, 2025</div>
			</div>
		</div>
	</ul>
	<ul class="pub-line"></ul>


	<!-- <ul class="pub-line"></ul> -->
	<!-- <ul class="pub-line"></ul> -->
	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://anttwo.github.io/milo/">
					<img class="pub-img" src="img/milo.gif" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://anttwo.github.io/milo/"> MILo: Mesh-In-the-Loop Gaussian Splatting
					for Detailed and Efficient Surface Reconstruction</a>
				<div class="pub-authors"> Antoine Guédon, Diego Gomez, <b>Nissim Maruani</b>, Bingchen Gong, George
					Drettakis, Maks Ovsjanikov </div>

				<div class="pub-abstract">
					Our method introduces a novel differentiable mesh extraction framework that operates during the
					optimization of 3D Gaussian Splatting representations. At every training iteration, we
					differentiably extract a mesh—including both vertex locations and connectivity—only from Gaussian
					parameters. This enables gradient flow from the mesh to Gaussians, allowing us to promote
					bidirectional consistency between volumetric (Gaussians) and surface (extracted mesh)
					representations. This approach guides Gaussians toward configurations better suited for surface
					reconstruction, resulting in higher quality meshes with significantly fewer vertices. Our framework
					can be plugged into any Gaussian splatting representation, increasing performance while generating
					an order of magnitude fewer mesh vertices. MILo makes the reconstructions more practical for
					downstream applications like physics simulations and animation.
				</div>
				<div class="pub-conf">ACM Trans. Graph. (SIGGRAPH Asia - <u>Journal Track</u>), 2025</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/projects/shapeshifter">
					<img class="pub-img" src="img/shapeshifter.gif" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/projects/shapeshifter"> ShapeShifter: 3D Variations
					Using Multiscale and Sparse Point-Voxel
					Diffusion</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Wang Yifan, Matthew Fisher, Pierre Alliez, Mathieu
					Desbrun </div>

				<div class="pub-abstract">
					This paper proposes a new 3D generative model that learns to synthesize shape variations based on a
					single
					example. While generative methods for 3D objects have recently attracted much attention, current
					techniques often lack geometric details and/or require long training times and large resources. Our
					approach remedies these issues by combining sparse voxel grids and multiscale point, normal, and
					color
					sampling within an encoder-free neural architecture that can be trained efficiently and in parallel.
					We
					show that our resulting variations better capture the fine details of their original input and can
					capture
					more general types of surfaces than previous SDF-based methods. Moreover, we offer interactive
					generation
					of 3D shape variants, allowing more human control in the design loop if needed.
				</div>
				<div class="pub-conf"> Proc. Conference on Computer Vision and Patter Recognition (CVPR), 2025</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">
		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/projects/ponq">
					<img class="pub-img" src="img/PoNQ.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/projects/ponq"> PoNQ: a Neural QEM-based Mesh
					Representation</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Maks Ovsjanikov, Pierre Alliez, Mathieu Desbrun </div>

				<div class="pub-abstract">
					Although polygon meshes have been a standard representation in geometry processing, their irregular
					and combinatorial nature hinders their suitability for learning-based applications. In this work, we
					introduce a novel learnable mesh representation through a set of local 3D sample Points and their
					associated Normals and Quadric error metrics (QEM) w.r.t. the underlying shape, which we denote
					PoNQ. A global mesh is directly derived from PoNQ by efficiently leveraging the knowledge of the
					local quadric errors. Besides marking the first use of QEM within a neural shape representation, our
					contribution guarantees both topological and geometrical properties by ensuring that a PoNQ mesh
					does not self-intersect and is always the boundary of a volume. Notably, our representation does not
					rely on a regular grid, is supervised directly by the target surface alone, and also handles open
					surfaces with boundaries and/or sharp features. We demonstrate the efficacy of PoNQ through a
					learning-based mesh prediction from SDF grids and show that our method surpasses recent
					state-of-the-art techniques in terms of both surface and edge-based metrics.
				</div>
				<div class="pub-conf"> Proc. Conference on Computer Vision and Patter Recognition (CVPR), 2024</div>
			</div>
		</div>
	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">

		<div class="container">
			<div class="zoomhover">
				<a href="https://nissmar.github.io/voromesh.github.io/">
					<img class="pub-img" src="img/Voromesh.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://nissmar.github.io/voromesh.github.io/"> VoroMesh: Learning Watertight
					Surface Meshes with Voronoi Diagrams</a>
				<div class="pub-authors"> <b>Nissim Maruani</b>, Roman Klokov, Maks Ovsjanikov, Pierre Alliez, Mathieu
					Desbrun </div>

				<div class="pub-abstract">
					In stark contrast to the case of images, finding a concise, learnable discrete representation of 3D
					surfaces remains a challenge. In particular, while polygon meshes are arguably the most common
					surface representation used in geometry processing, their irregular and combinatorial structure
					often make them unsuitable for learning-based applications. In this work, we present VoroMesh, a
					novel and differentiable Voronoi-based representation of water- tight 3D shape surfaces. From a set
					of 3D points (called generators) and their associated occupancy, we define our boundary
					representation through the Voronoi diagram of the generators as the subset of Voronoi faces whose
					two associated (equidistant) generators are of opposite occupancy: the resulting polygon mesh forms
					a watertight approximation of the target shape’s boundary. To learn the position of the generators,
					we propose a novel loss function, dubbed VoroLoss, that minimizes the distance from groundtruth
					surface samples to the closest faces of the Voronoi diagram which does not require an explicit
					construction of the entire Voronoi diagram. A direct optimization of the Voroloss to obtain
					generators on the Thingi32 dataset demonstrates the geometric efficiency of our representation
					compared to axiomatic meshing algorithms and recent learning-based mesh representations. We further
					use VoroMesh in a learning-based mesh prediction task from input SDF grids on the ABC dataset, and
					show comparable performance to state-of-the-art methods while guaranteeing closed output surfaces
					free of self-intersections.
				</div>
				<div class="pub-conf"> Proc. International Conference on Computer Vision (ICCV), 2023</div>

			</div>
		</div>

	</ul>

	<ul class="pub-line"></ul>

	<ul class="pub">

		<div class="container">
			<div class="zoomhover">
				<a href="https://inria.hal.science/hal-04135266#">
					<img class="pub-img" src="img/offset.png" alt="Publication 1 Image">
				</a>
			</div>
			<div>
				<a class="pub-title" href="https://inria.hal.science/hal-04135266#"> Feature-Preserving Offset Mesh
					Generation from Topology-Adapted Octrees</a>
				<div class="pub-authors">Daniel Zint, <b>Nissim Maruani</b>, Mael Rouxel-Labbé, Pierre Alliez</div>

				<div class="pub-abstract">We introduce a reliable method to generate offset meshes from input triangle
					meshes or triangle soups. Our method proceeds in two steps. The first step performs a Dual
					Contouring method on the offset surface, operating on an adaptive octree that is refined in areas
					where the offset topology is complex. Our approach substantially reduces memory consumption and
					runtime compared to isosurfacing methods operating on uniform grids. The second step improves the
					output Dual Contouring mesh with an offset-aware remeshing algorithm to reduce the normal deviation
					between the mesh facets and the exact offset. This remeshing process reconstructs concave sharp
					features and approximates smooth shapes in convex areas up to a user-defined precision. We show the
					effectiveness and versatility of our method by applying it to a wide range of input meshes. We also
					benchmark our method on the entire Thingi10k dataset: watertight, 2-manifold offset meshes are
					obtained for 100% of the cases.</div>
				<div class="pub-conf">Symposium on geometry processing (SGP), 2023</div>

			</div>

		</div>
	</ul>
	<!-- <ul class="pub-line"></ul> -->

	<h2>Teaching</h2>
	<ul>
		<li> ENS-PSL (2022-Today, Paris, France): exploring new ways of teaching maths with <a class="custom-link"
				href="https://mathadata.fr/">MathAData</a> </li>
		<li> Polytech Nice (November 2025): Lecture & practical session on 3D Shape Learning within the Deep Learning
			class</li>
		<li>
			<a class="custom-link" href="https://www.larotonde-sciences.com">La Rotonde</a> (2018-2019, Saint-Étienne,
			France): Science Mediation at <a class="custom-link" href="https://fondation-lamap.org/en">La main à la
				pâte</a>
		</li>
	</ul>

	<h2>Talks</h2>
	<ul>
		<li> Stanford, Geometric Computation group, July 2024</li>
		<li> 3IA Côte d'Azur, June 2023, November 2025</li>
	</ul>

	<h2>Reviewer</h2>
	<ul>
		<li> Computers & Graphics 2026, EUROGRAPHICS 2026, BMVC 2025, SIGGRAPH 2025, BMVC 2024, SIGGRAPH ASIA 2024,
			SIGGRAPH 2024, BMVC 2023</li>
	</ul>

	<h2>Education</h2>
	<ul>
		<li> ENS Paris-Saclay (2021-2022, Gif-sur-Yvette, France): Master MVA</li>
		<li> École polytechnique (2018-2022, Saclay, France): Engineering Curriculum </li>
		<li> Lycée Louis-le-Grand (2016-2018, Paris, France): "Classe prépa" </li>
	</ul>


	<h2>Coding projects</h2>

	<div class="container project">
		<a href="https://github.com/nissmar/Bouncing_Lasers" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/lasers.gif">
			<p class="code-title custom-link">Laser Simulation</p>
		</a>
		<a href="https://github.com/nissmar/ForgeryDetection" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/forgery.gif">
			<p class="code-title custom-link">Forgery Detection</p>
		</a>
		<a href="https://github.com/nissmar/VSA" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/vsa.gif">
			<p class="code-title custom-link">VSA</p>
		</a>
		<a href="https://github.com/nissmar/Learning-Active-Contour" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/segmentation.gif">
			<p class="code-title custom-link">Image Segmentation</p>
		</a>
		<a href="https://github.com/nissmar/Radiance-Fields" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/plenoxels.gif">
			<p class="code-title custom-link">Plenoxels</p>
		</a>
		<a href="https://github.com/nissmar/ARDM" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/ardm.jpg">
			<p class="code-title custom-link">Autoregressive Diffusion</p>
		</a>
		<a href="https://github.com/nissmar/FilmNoise/" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/film-noise.jpg">
			<p class="code-title custom-link">Analog Grain Simulation</p>
		</a>
		<a href="https://github.com/nissmar/Paper_Plane_VCL/" class="code-project zoomhover">
			<img class="code-img" src="img/coding_projects/airplane.png">
			<p class="code-title custom-link">Paper Plane Simulation</p>
		</a>





	</div>



</body>

</html>